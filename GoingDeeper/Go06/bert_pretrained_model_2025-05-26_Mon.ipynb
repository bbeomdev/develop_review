{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b43deae",
   "metadata": {},
   "source": [
    "# mini BERT 만들기\n",
    "\n",
    "vocab size를 8000으로 줄이고, 전체 파라미터 사이즈가 1M 정도가 되는 아주 작은 mini BERT 모델을 만들어 10 Epoch까지 학습시킨 모델을 만들어 보는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27061140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b4a1f2",
   "metadata": {},
   "source": [
    "## Tokenizer 준비\n",
    "\n",
    "\n",
    "SentencePiece 모델을 이용해 BERT의 MLM 학습용 데이터를 만드세요.\n",
    "\n",
    "이를 위해 한글 나무 위키 코퍼스로부터 8000의 vocab_size를 갖는 sentencepiece 모델을 만들어 보세요. BERT에 사용되는 주요 특수문자가 vocab에 포함되어야 합니다. (시간이 부족하다면 클라우드에 저장된 sentencepiece 모델을 사용하세요.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76674b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/bert_pretrain/data/kowiki.txt --model_prefix=/workspace/userdisk/project/GoingDeeper/Go06/ko_8000 --vocab_size=8007 --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "  input_format: \n",
      "  model_prefix: /workspace/userdisk/project/GoingDeeper/Go06/ko_8000\n",
      "  model_type: BPE\n",
      "  vocab_size: 8007\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [SEP]\n",
      "  user_defined_symbols: [CLS]\n",
      "  user_defined_symbols: [MASK]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (2451287), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2451287 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=287452241\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=4411\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2450254 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2450254\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 7050692\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1781571 min_freq=424\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=576838 size=20 all=581927 active=38577 piece=▁아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390836 size=40 all=591445 active=48095 piece=▁유\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=297873 size=60 all=601378 active=58028 piece=에는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=244712 size=80 all=609974 active=66624 piece=▁성\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=194372 size=100 all=616449 active=73099 piece=까지\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=193674 min_freq=462\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=176838 size=120 all=625299 active=38770 piece=▁우\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=154294 size=140 all=632274 active=45745 piece=▁파\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140625 size=160 all=639734 active=53205 piece=00\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125983 size=180 all=645481 active=58952 piece=▁요\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114855 size=200 all=649839 active=63310 piece=리아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=114086 min_freq=457\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=106760 size=220 all=657338 active=39316 piece=▁같은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100770 size=240 all=662564 active=44542 piece=▁왕\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96037 size=260 all=670536 active=52514 piece=▁목\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=88392 size=280 all=675441 active=57419 piece=▁f\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=81678 size=300 all=681701 active=63679 piece=▁선수\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=80870 min_freq=446\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=77144 size=320 all=686930 active=39163 piece=▁때문에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=73218 size=340 all=691000 active=43233 piece=▁조선\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=68829 size=360 all=695717 active=47950 piece=▁천\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=64009 size=380 all=700839 active=53072 piece=▁196\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=60953 size=400 all=706675 active=58908 piece=▁돌\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=60762 min_freq=435\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=58542 size=420 all=711350 active=39712 piece=▁다시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=55779 size=440 all=715377 active=43739 piece=▁K\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52528 size=460 all=721839 active=50201 piece=▁모두\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=49784 size=480 all=727356 active=55718 piece=▁히\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47637 size=500 all=733038 active=61400 piece=▁전쟁\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=47558 min_freq=423\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=45919 size=520 all=738287 active=41703 piece=▁있어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=44025 size=540 all=743886 active=47302 piece=▁중심\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42378 size=560 all=748357 active=51773 piece=▁N\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=40922 size=580 all=752114 active=55530 piece=▁H\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39922 size=600 all=755142 active=58558 piece=le\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39768 min_freq=414\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38924 size=620 all=761204 active=43650 piece=▁검\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=37771 size=640 all=767376 active=49822 piece=란드\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36292 size=660 all=772837 active=55283 piece=정을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=35134 size=680 all=778715 active=61161 piece=▁설립\n",
      "bpe_model_trainer.cc(258) LOG(INFO) A"
     ]
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "spm_model = os.getcwd()+'/ko_8000'\n",
    "vocab_size = 8000\n",
    "spm.SentencePieceTrainer.train(f\"--input={corpus_file} --model_prefix={spm_model} --vocab_size={vocab_size + 7} --model_type=bpe --max_sentence_length=999999 --pad_id=0 --pad_piece=[PAD] --unk_id=1 --unk_piece=[UNK] --bos_id=2 --bos_piece=[BOS] --eos_id=3 --eos_piece=[EOS] --user_defined_symbols=[SEP],[CLS],[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79623218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dded: freq=34016 size=700 all=783719 active=66165 piece=▁역사\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=34003 min_freq=400\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=33144 size=720 all=787243 active=42507 piece=▁만들어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32383 size=740 all=791538 active=46802 piece=▁시간\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31645 size=760 all=795131 active=50395 piece=▁측\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30941 size=780 all=798845 active=54109 piece=과의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30041 size=800 all=803050 active=58314 piece=도는\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=30023 min_freq=392\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=29448 size=820 all=808546 active=45099 piece=▁난\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28836 size=840 all=813895 active=50448 piece=▁21\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28270 size=860 all=818654 active=55207 piece=▁찾\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=27299 size=880 all=824625 active=61177 piece=되지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26862 size=900 all=828285 active=64837 piece=하자\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=26833 min_freq=381\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=26060 size=920 all=832595 active=45199 piece=부에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=25504 size=940 all=838824 active=51428 piece=수의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24726 size=960 all=843322 active=55926 piece=▁남아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=24083 size=980 all=848416 active=61020 piece=▁않는다\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23647 size=1000 all=853601 active=66205 piece=인민\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=23602 min_freq=368\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=23176 size=1020 all=859529 active=48367 piece=▁마지\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22849 size=1040 all=863129 active=51967 piece=▁시리즈\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=22293 size=1060 all=868678 active=57516 piece=제로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21851 size=1080 all=873075 active=61913 piece=시의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21377 size=1100 all=878277 active=67115 piece=해야\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=21369 min_freq=355\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=21043 size=1120 all=882003 active=47093 piece=▁녹\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20625 size=1140 all=886266 active=51356 piece=▁인구는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=20185 size=1160 all=889382 active=54472 piece=ac\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19762 size=1180 all=894738 active=59828 piece=인은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19447 size=1200 all=899565 active=64655 piece=50\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=19432 min_freq=347\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=19118 size=1220 all=903215 active=48471 piece=광역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18777 size=1240 all=908007 active=53263 piece=수는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18396 size=1260 all=913268 active=58524 piece=인민공\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=18040 size=1280 all=917389 active=62645 piece=▁긴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17659 size=1300 all=922177 active=67433 piece=▁전통\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=17629 min_freq=335\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17404 size=1320 all=926354 active=50086 piece=▁망\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=17093 size=1340 all=929494 active=53226 piece=단의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16931 size=1360 all=934269 active=58001 piece=▁인간\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16719 size=1380 all=939872 active=63604 piece=▁바로\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16586 size=1400 all=943447 active=67179 piece=▁탑\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=16584 min_freq=327\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16331 size=1420 all=947545 active=51069 piece=▁태양\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=16086 size=1440 all=952041 active=55565 piece=▁경기에서\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15873 size=1460 all=957568 active=61092 piece=▁동일\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15577 size=1480 all=962681 active=66205 piece=드를\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15352 size=1500 all=966310 active=69834 piece=▁뮤\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=15328 min_freq=318\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=15103 size=1520 all=968851 active=50741 piece=id\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14971 size=1540 all=972545 active=54435 piece=▁옥\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14723 size=1560 all=977306 active=59196 piece=비전\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14495 size=1580 all=982182 active=64072 piece=▁대부분의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14292 size=1600 all=985970 active=67860 piece=▁갑\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=14280 min_freq=311\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=14118 size=1620 all=990010 active=53173 piece=회는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13982 size=1640 all=993514 active=56677 piece=▁사고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13729 size=1660 all=997224 active=60387 piece=cm\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13567 size=1680 all=1001600 active=64762 piece=▁뛰어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13433 size=1700 all=1006574 active=69736 piece=▁더욱\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=13420 min_freq=303\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=13135 size=1720 all=1011076 active=54820 piece=지역\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12940 size=1740 all=1015431 active=59175 piece=▁선언\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12821 size=1760 all=1020339 active=64083 piece=▁어려\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12640 size=1780 all=1023221 active=66965 piece=▁칭\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12505 size=1800 all=1028177 active=71921 piece=▁옛\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=12503 min_freq=295\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12409 size=1820 all=1033793 active=56931 piece=ce\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12253 size=1840 all=1037183 active=60321 piece=▁그들의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=12091 size=1860 all=1041900 active=65038 piece=▁단체\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11934 size=1880 all=1045379 active=68517 piece=▁예술\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11799 size=1900 all=1048007 active=71145 piece=라의\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11799 min_freq=288\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11693 size=1920 all=1050313 active=54358 piece=▁불구하고\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11594 size=1940 all=1053703 active=57748 piece=▁분리\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11496 size=1960 all=1057998 active=62043 piece=▁사이의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11381 size=1980 all=1063418 active=67463 piece=단이\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11289 size=2000 all=1067885 active=71930 piece=im\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=11277 min_freq=281\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=11133 size=2020 all=1071888 active=57167 piece=▁등과\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10994 size=2040 all=1077153 active=62432 piece=법을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10895 size=2060 all=1081334 active=66613 piece=▁괴\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10776 size=2080 all=1084885 active=70164 piece=러스\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10692 size=2100 all=1089330 active=74609 piece=▁깨\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=10688 min_freq=274\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10597 size=2120 all=1091576 active=56585 piece=릭터\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10496 size=2140 all=1095180 active=60189 piece=▁확장\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10396 size=2160 all=1100510 active=65519 piece=었던\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=10271 size=2180 all=1104334 active=69343 piece=▁남부\n",
      "bpe_model_train"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{spm_model}.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6131a35",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (1) MASK 생성\n",
    "\n",
    "BERT의 MLM에 필요한 빈칸(mask)을 학습 데이터 전체 토큰의 15% 정도로 만들어 주세요. 그 중 80%는 [MASK] 토큰, 10%는 랜덤한 토큰, 나머지 10%는 원래의 토큰을 그대로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbb730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    # 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "            \n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "    \n",
    "    # masking\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    \n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23e9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc6687",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (2) NSP pair 생성\n",
    "\n",
    "BERT의 pretrain task인 NSP는 두 문장이 연속하는지 확인하는 것입니다. 이를 위해 2개의 문장을 짝지어 50%의 확률로 TRUE와 FALSE를 지정해 주세요.\n",
    "\n",
    "두 문장 사이에 segment 처리를 해주세요. 첫 번째 문장의 segment는 0, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주세요.\n",
    "\n",
    "MLM과 NSP는 동시에 학습된다는 것을 염두에 두고 학습 데이터를 구성해 보세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a057be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 위 코드들을 참고하여 아래 함수를 완성시켜주세요.\n",
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):  # doc 전체를 loop\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "#             print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "#             print(\"is_next:\", is_next)\n",
    "#             print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "#             print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "            #######################################\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "#             print(\"tokens:\", len(tokens), tokens)\n",
    "#             print(\"segment:\", len(segment), segment)\n",
    "            \n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "#             print(\"masked tokens:\", len(tokens), tokens)\n",
    "#             print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "#             print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "            #######################################\n",
    "#             print()\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d185fcc",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (3) 데이터셋 완성\n",
    "\n",
    "BERT pretrain 데이터셋을 생성해, json 포맷으로 저장하세요. 데이터셋의 사이즈가 크므로np.memmap을 사용해 메모리 사용량을 최소화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec8183e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf334d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. 아래 주석에 따라 코드를 완성해주세요.\n",
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if len(doc) > 0:\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []  # doc 초기화\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if len(pieces) > 0:\n",
    "                        doc.append(pieces)                    \n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b9385ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93f944d5b1a44b694d300b721d6cc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getcwd()+'/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "967870aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918189"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json 확인 & 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4efeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c039b146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cedcd7dbe443e6b0c5afb0e926b8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_634/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_634/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_634/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f9bc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   10, 1605, 3599, 1755, 3630,   41, 3644,  830, 3624, 1135,\n",
       "           52, 3599,   13,   81,   87, 1501, 2247,   25, 3779, 3873, 3667,\n",
       "         3631, 3813, 3873, 4196, 3636, 3779, 3601,  249, 3725, 1232,   33,\n",
       "           52, 3599,    6,    6,    6, 6322, 2780,   14, 1509,  168, 3877,\n",
       "          414,  165, 1697, 4290, 3873, 3703, 3683,  593,   21, 5007,  399,\n",
       "         1927, 3607,    6,    6,    6,    6,    6,    6,  103, 4313, 4290,\n",
       "          613, 3638, 3718,   98, 3878, 3656,  256, 2543,  309,  337, 3735,\n",
       "          181, 3616, 3603,  489,  376, 3599,    4,    6,    6,  207, 3714,\n",
       "            6, 1042,  103, 3610, 3686, 3718,    6,    6,   37, 3418,  416,\n",
       "          810, 3666, 3625,  131, 3662,    7, 3629,  203,  241, 3602, 1114,\n",
       "         3724,  788,  243,    6,    6,    6,  663, 1647, 3682, 3682, 3625,\n",
       "          203, 3008, 3625, 3616,   16, 3599,    4], dtype=int32),\n",
       " memmap([   5, 3676,  848, 3784, 1931,   58, 3676,  416, 2316, 3619, 3625,\n",
       "         3617, 3744, 4335,   12, 3625, 3616,  175, 3662,    7, 3629,  203,\n",
       "            6,    6,    6,    6,    6,    6,  143, 3625, 3616,  131, 3662,\n",
       "          342, 3629, 3616, 3602,  176,  334,  829, 1115, 3665,    6,    6,\n",
       "         3451, 1633,  375,  671, 1644, 3608,  547, 3423,  765,  815, 3604,\n",
       "            6,    6,    6, 2375, 3608, 3604,  532, 2589, 3599,    4,  307,\n",
       "          323,    6,  321, 3611,  622,  122, 3725, 3620, 3627, 3837, 3608,\n",
       "            6,  176,  268, 4082,   94,  567, 4014, 3617, 7474, 3616, 3830,\n",
       "           66, 3590,  307,  192, 1272,  158, 3788,  353, 3599,  202,  316,\n",
       "         3600,  176,   10,  323,  476, 3663, 1329,  605,  238, 3631, 2470,\n",
       "         3604, 1939,  106, 3627,   13,    6,    6, 1128,   48,    6,    6,\n",
       "          848, 3784, 3833,    8, 3637, 2263,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 0,\n",
       " 1,\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  479, 3652, 3625,  243,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  813,   17, 3599,  307,  587,  931,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,   18, 3686,    0,    0,\n",
       "         3324,    0,    0,    0,    0,    0,  207, 3714,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,   49, 3632,  796,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          578, 3652, 3625, 3617, 4148, 3665,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1381, 4148,\n",
       "         3451,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          752, 3608, 3604,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2143,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          347,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  162,  490,    0,    0,   28, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fe61b",
   "metadata": {},
   "source": [
    "## BERT 모델 구현\n",
    "\n",
    "pad mask, ahead mask 함수, gelu activation 함수, parameter initializer 생성 함수, json을 config 형태로 사용하기 위한 유틸리티 함수를 먼저 만들어 두세요.\n",
    "\n",
    "Embedding 레이어, Transformer encoder 레이어, BERT 레이어를 구성한 후, pretraine용 BERT 모델을 만들어 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cded64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0fe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d173578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f056f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac35c0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6ea0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf9c72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fbac63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccc9fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e98f8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "157d65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af2afbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be26d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8060817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3b28a",
   "metadata": {},
   "source": [
    "## pretrain 진행\n",
    "\n",
    "loss, accuracy 함수를 정의하고 Learning Rate 스케쥴링을 구현한 후, 10 Epoch까지 모델 학습을 시켜보세요. 학습을 진행할 때는 배치 사이즈에 유의하세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbc14371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7826585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a106906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad5c58a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4485632     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7a1f508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43635ea",
   "metadata": {},
   "source": [
    "## 프로젝트 결과\n",
    "\n",
    "학습된 모델과 학습과정을 시각화해 보세요. NSP와 MLM의 loss가 안정적으로 수렴하나요? 모델이 작기 때문에 loss가 잘 수렴하지 않을 수도 있어요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c602355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 267s 131ms/step - loss: 19.5987 - nsp_loss: 0.6503 - mlm_loss: 18.9484 - nsp_acc: 0.5899 - mlm_lm_acc: 0.1099\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.10990, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 264s 132ms/step - loss: 17.5316 - nsp_loss: 0.6226 - mlm_loss: 16.9090 - nsp_acc: 0.6176 - mlm_lm_acc: 0.1294\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.10990 to 0.12936, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 265s 132ms/step - loss: 16.4406 - nsp_loss: 0.6155 - mlm_loss: 15.8252 - nsp_acc: 0.6240 - mlm_lm_acc: 0.1434\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.12936 to 0.14335, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 265s 132ms/step - loss: 14.4186 - nsp_loss: 0.6136 - mlm_loss: 13.8050 - nsp_acc: 0.6275 - mlm_lm_acc: 0.1818\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.14335 to 0.18178, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 265s 132ms/step - loss: 13.5347 - nsp_loss: 0.6085 - mlm_loss: 12.9262 - nsp_acc: 0.6370 - mlm_lm_acc: 0.2062\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.18178 to 0.20617, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 265s 133ms/step - loss: 13.0375 - nsp_loss: 0.6031 - mlm_loss: 12.4345 - nsp_acc: 0.6459 - mlm_lm_acc: 0.2216\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.20617 to 0.22155, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 266s 133ms/step - loss: 12.7118 - nsp_loss: 0.5981 - mlm_loss: 12.1137 - nsp_acc: 0.6570 - mlm_lm_acc: 0.2323\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.22155 to 0.23233, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 265s 133ms/step - loss: 12.4929 - nsp_loss: 0.5920 - mlm_loss: 11.9009 - nsp_acc: 0.6686 - mlm_lm_acc: 0.2398\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.23233 to 0.23977, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 265s 133ms/step - loss: 12.3642 - nsp_loss: 0.5874 - mlm_loss: 11.7768 - nsp_acc: 0.6771 - mlm_lm_acc: 0.2442\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.23977 to 0.24420, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 265s 133ms/step - loss: 12.3053 - nsp_loss: 0.5849 - mlm_loss: 11.7204 - nsp_acc: 0.6814 - mlm_lm_acc: 0.2461\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.24420 to 0.24607, saving model to /workspace/userdisk/project/GoingDeeper/Go06/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_dir=os.getcwd()\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(\n",
    "    x=pre_train_inputs,               # 입력 (tokens, segments)\n",
    "    y=pre_train_labels,               # 정답 (labels_nsp, labels_mlm)\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[save_weights]\n",
    ")\n",
    "# 모델 인자에는 inputs, labels, epochs, batch size, callback 이 필요해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5210e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84060ed6",
   "metadata": {},
   "source": [
    "mlm의 loss는 줄어들지 않은 것을 확인했다.  \n",
    "rep의 loss는 줄어들다가 수렴하는 것으로 보인다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92ad6c",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2e789",
   "metadata": {},
   "source": [
    "과학 기술 기사\n",
    "```\n",
    "자기장이 없는 달 뒷면의 일부 지역 암석에 강한 자기장 흔적이 남아 있는 궁금증을 해소하는 연구 결과가 나와 주목된다. 과거 대형 소행성 충돌에 따른 결과라는 분석이다.\n",
    "```\n",
    "\n",
    "```\n",
    "달 자체에는 원래 자기장이 없지만, 궤도탐사선 관측 등에서는 뒷면의 암석에서 강한 자기장 흔적이 포착되고 있다. 1960~1970년대 미 항공우주국(NASA) 아폴로 임무 이후 이런 사실이 확인됐지만, 그동안 정확한 원인은 밝혀지지 않았다. 이를 설명하기 위해 달 내부에 지구처럼 용융상태 금속이 있어 자기장을 형성하는 ‘다이너모 이론’(Dynamo theory)을 적용한 가설이 제시되기도 했다. 그러나 핵이 너무 작고 특히 뒷면 암석의 강한 자기를 설명하기에는 부족하다는 지적이 제기됐다. 이 연구의 공동 연구자인 MIT 로나 오란 교수와 벤저민 와이스 교수팀이 대안 가설로 거대한 충돌이 플라스마를 생성하고, 그것이 태양 자기장을 증폭시켜 표면 암석을 자화시켰다는 가설을 내놓기도 했으나 이 역시 가능성이 작은 것으로 밝혀졌다.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f8bf6",
   "metadata": {},
   "source": [
    "### NSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "13abb8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def inference_nsp(text_a, text_b):\n",
    "    tokens_a = vocab.encode_as_ids(text_a)\n",
    "    tokens_b = vocab.encode_as_ids(text_b)\n",
    "\n",
    "    tokens = [vocab.bos_id()] + tokens_a + [vocab.eos_id()] + tokens_b + [vocab.eos_id()]\n",
    "    segments = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    # padding\n",
    "    tokens = pad_sequences([tokens], maxlen=config.n_seq, padding='post')\n",
    "    segments = pad_sequences([segments], maxlen=config.n_seq, padding='post')\n",
    "    \n",
    "    outputs_nsp, _ = pre_train_model.predict([tokens, segments])\n",
    "    \n",
    "    nsp_label = np.argmax(outputs_nsp, axis=-1)[0]\n",
    "    print(\"NSP 예측:\", \"문장 연결 O\" if nsp_label == 1 else \"문장 연결 X\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "185ba2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSP 예측: 문장 연결 X\n"
     ]
    }
   ],
   "source": [
    "text_a = \"자기장이 없는 달 뒷면의 일부 지역 암석에 강한 자기장 흔적이 남아 있는 궁금증을 해소하는 연구 결과가 나와 주목된다.\"\n",
    "text_b = \"과거 대형 소행성 충돌에 따른 결과라는 분석이다.\"\n",
    "\n",
    "inference_nsp(text_a, text_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b6975d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "달 자체에는 원래 자기장이 없지만, 궤도탐사선 관측 등에서는 뒷면의 암석에서 강한 자기장 흔적이 포착되고 있다\n",
      "1960~1970년대 미 항공우주국(NASA) 아폴로 임무 이후 이런 사실이 확인됐지만, 그동안 정확한 원인은 밝혀지지 않았다\n",
      "NSP 예측: 문장 연결 O\n",
      "\n",
      "\n",
      "1960~1970년대 미 항공우주국(NASA) 아폴로 임무 이후 이런 사실이 확인됐지만, 그동안 정확한 원인은 밝혀지지 않았다\n",
      "이를 설명하기 위해 달 내부에 지구처럼 용융상태 금속이 있어 자기장을 형성하는 ‘다이너모 이론’(Dynamo theory)을 적용한 가설이 제시되기도 했다\n",
      "NSP 예측: 문장 연결 X\n",
      "\n",
      "\n",
      "이를 설명하기 위해 달 내부에 지구처럼 용융상태 금속이 있어 자기장을 형성하는 ‘다이너모 이론’(Dynamo theory)을 적용한 가설이 제시되기도 했다\n",
      "그러나 핵이 너무 작고 특히 뒷면 암석의 강한 자기를 설명하기에는 부족하다는 지적이 제기됐다\n",
      "NSP 예측: 문장 연결 X\n",
      "\n",
      "\n",
      "그러나 핵이 너무 작고 특히 뒷면 암석의 강한 자기를 설명하기에는 부족하다는 지적이 제기됐다\n",
      "이 연구의 공동 연구자인 MIT 로나 오란 교수와 벤저민 와이스 교수팀이 대안 가설로 거대한 충돌이 플라스마를 생성하고, 그것이 태양 자기장을 증폭시켜 표면 암석을 자화시켰다는 가설을 내놓기도 했으나 이 역시 가능성이 작은 것으로 밝혀졌다.\n",
      "NSP 예측: 문장 연결 O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraph = \"달 자체에는 원래 자기장이 없지만, 궤도탐사선 관측 등에서는 뒷면의 암석에서 강한 자기장 흔적이 포착되고 있다. 1960~1970년대 미 항공우주국(NASA) 아폴로 임무 이후 이런 사실이 확인됐지만, 그동안 정확한 원인은 밝혀지지 않았다. 이를 설명하기 위해 달 내부에 지구처럼 용융상태 금속이 있어 자기장을 형성하는 ‘다이너모 이론’(Dynamo theory)을 적용한 가설이 제시되기도 했다. 그러나 핵이 너무 작고 특히 뒷면 암석의 강한 자기를 설명하기에는 부족하다는 지적이 제기됐다. 이 연구의 공동 연구자인 MIT 로나 오란 교수와 벤저민 와이스 교수팀이 대안 가설로 거대한 충돌이 플라스마를 생성하고, 그것이 태양 자기장을 증폭시켜 표면 암석을 자화시켰다는 가설을 내놓기도 했으나 이 역시 가능성이 작은 것으로 밝혀졌다.\"\n",
    "paragraph_list = paragraph.split(\". \")\n",
    "for i in range(len(paragraph_list) -1):\n",
    "    print(paragraph_list[i])\n",
    "    print(paragraph_list[i+1])\n",
    "    inference_nsp(paragraph_list[i],paragraph_list[i+1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c13f43",
   "metadata": {},
   "source": [
    "비즈니스 기사\n",
    "\n",
    "오픈AI가 한국에 법인을 설립하고 국내 공식 진출한다. 인공지능(AI) 기술 수용성과 산업 잠재력이 높은 한국을 글로벌 협력의 핵심 거점으로 삼고, 정부 및 기업과의 인프라 구축·모델 개발 협력에 박차를 가하겠다는 방침이다.\n",
    "\n",
    "한국을 찾은 제이슨 권 오픈AI 최고전략책임자(CSO)는 26일 서울 종로구 포시즌스 호텔에서 기자들과 만나 향후 몇 달 내 서울에 한국 법인을 설립하고, 각국 현지에서 AI 생태계 협력을 강화하는 ‘오픈AI 포 컨트리즈’ 프로그램을 한국에 적용하겠다는 계획을 밝혔다.\n",
    "\n",
    "서울에 설립되는 오픈AI 한국 법인은 글로벌 11번째 지사로 아시아에서는 일본과 싱가포르에 이어 세 번째 사무소다. 오픈AI는 한국 고객 지원, 기업 협업, 공공 파트너십 등을 위한 현지 인력을 채용할 계획이다. 구체적인 인력 규모 등은 공개되지 않았으나 곧 구직 페이지에 공지될 예정이다.\n",
    "\n",
    "권 CSO는 “한국은 AI 기술 수용 속도가 세계 최고 수준으로 미국 다음으로 챗GPT 유료 구독자를 많이 보유한 국가이며, 특히 서울은 영상 생성 모델 ‘소라’ 사용률이 전 세계 1위인 도시”라며 “현지 법인을 중심으로 파트너십을 강화하고 한국 맞춤형 AI를 함께 만들고자 한다”고 말했다.\n",
    "\n",
    "오픈AI는 한국 법인 설립과 동시에 최근 전세계 각국에서 추진하고 있는 ‘오픈AI 포 컨트리즈’ 프로그램도 한국에 본격 적용할 방침이다. 오픈AI 포 컨트리즈는 아랍에미리트(UAE) 아부다비에 5기가와트(GW) 규모의 데이터 센터 클러스터를 짓는 등 대규모 AI 인프라를 구축하는 ‘스타게이트’ 프로젝트 해외 확장 프로그램이다. 이 프로그램을 통해 오픈AI는 각국 정부 및 산업 파트너와 함께 데이터 주권, 현지 인프라 구축, AI 모델 현지화, 스타트업 생태계 육성 등을 포괄적으로 지원한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4032df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    # \". \" 또는 \".\\n\" 로 분리하되, 구분자 뒤의 공백도 제거\n",
    "    sentences = re.split(r'\\.\\s+|\\.\\n', text.strip())\n",
    "    # 빈 문자열 제거하고 마침표 붙여서 복원\n",
    "    return [s.strip() + '.' for s in sentences if s.strip()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cd785e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오픈AI가 한국에 법인을 설립하고 국내 공식 진출한다.\n",
      "인공지능(AI) 기술 수용성과 산업 잠재력이 높은 한국을 글로벌 협력의 핵심 거점으로 삼고, 정부 및 기업과의 인프라 구축·모델 개발 협력에 박차를 가하겠다는 방침이다.\n",
      "NSP 예측: 문장 연결 O\n",
      "\n",
      "\n",
      "인공지능(AI) 기술 수용성과 산업 잠재력이 높은 한국을 글로벌 협력의 핵심 거점으로 삼고, 정부 및 기업과의 인프라 구축·모델 개발 협력에 박차를 가하겠다는 방침이다.\n",
      "한국을 찾은 제이슨 권 오픈AI 최고전략책임자(CSO)는 26일 서울 종로구 포시즌스 호텔에서 기자들과 만나 향후 몇 달 내 서울에 한국 법인을 설립하고, 각국 현지에서 AI 생태계 협력을 강화하는 ‘오픈AI 포 컨트리즈’ 프로그램을 한국에 적용하겠다는 계획을 밝혔다.\n",
      "NSP 예측: 문장 연결 O\n",
      "\n",
      "\n",
      "한국을 찾은 제이슨 권 오픈AI 최고전략책임자(CSO)는 26일 서울 종로구 포시즌스 호텔에서 기자들과 만나 향후 몇 달 내 서울에 한국 법인을 설립하고, 각국 현지에서 AI 생태계 협력을 강화하는 ‘오픈AI 포 컨트리즈’ 프로그램을 한국에 적용하겠다는 계획을 밝혔다.\n",
      "서울에 설립되는 오픈AI 한국 법인은 글로벌 11번째 지사로 아시아에서는 일본과 싱가포르에 이어 세 번째 사무소다.\n",
      "NSP 예측: 문장 연결 X\n",
      "\n",
      "\n",
      "서울에 설립되는 오픈AI 한국 법인은 글로벌 11번째 지사로 아시아에서는 일본과 싱가포르에 이어 세 번째 사무소다.\n",
      "오픈AI는 한국 고객 지원, 기업 협업, 공공 파트너십 등을 위한 현지 인력을 채용할 계획이다.\n",
      "NSP 예측: 문장 연결 O\n",
      "\n",
      "\n",
      "오픈AI는 한국 고객 지원, 기업 협업, 공공 파트너십 등을 위한 현지 인력을 채용할 계획이다.\n",
      "구체적인 인력 규모 등은 공개되지 않았으나 곧 구직 페이지에 공지될 예정이다.\n",
      "NSP 예측: 문장 연결 X\n",
      "\n",
      "\n",
      "구체적인 인력 규모 등은 공개되지 않았으나 곧 구직 페이지에 공지될 예정이다.\n",
      "권 CSO는 “한국은 AI 기술 수용 속도가 세계 최고 수준으로 미국 다음으로 챗GPT 유료 구독자를 많이 보유한 국가이며, 특히 서울은 영상 생성 모델 ‘소라’ 사용률이 전 세계 1위인 도시”라며 “현지 법인을 중심으로 파트너십을 강화하고 한국 맞춤형 AI를 함께 만들고자 한다”고 말했다.\n",
      "NSP 예측: 문장 연결 O\n",
      "\n",
      "\n",
      "권 CSO는 “한국은 AI 기술 수용 속도가 세계 최고 수준으로 미국 다음으로 챗GPT 유료 구독자를 많이 보유한 국가이며, 특히 서울은 영상 생성 모델 ‘소라’ 사용률이 전 세계 1위인 도시”라며 “현지 법인을 중심으로 파트너십을 강화하고 한국 맞춤형 AI를 함께 만들고자 한다”고 말했다.\n",
      "오픈AI는 한국 법인 설립과 동시에 최근 전세계 각국에서 추진하고 있는 ‘오픈AI 포 컨트리즈’ 프로그램도 한국에 본격 적용할 방침이다.\n",
      "NSP 예측: 문장 연결 X\n",
      "\n",
      "\n",
      "오픈AI는 한국 법인 설립과 동시에 최근 전세계 각국에서 추진하고 있는 ‘오픈AI 포 컨트리즈’ 프로그램도 한국에 본격 적용할 방침이다.\n",
      "오픈AI 포 컨트리즈는 아랍에미리트(UAE) 아부다비에 5기가와트(GW) 규모의 데이터 센터 클러스터를 짓는 등 대규모 AI 인프라를 구축하는 ‘스타게이트’ 프로젝트 해외 확장 프로그램이다.\n",
      "NSP 예측: 문장 연결 X\n",
      "\n",
      "\n",
      "오픈AI 포 컨트리즈는 아랍에미리트(UAE) 아부다비에 5기가와트(GW) 규모의 데이터 센터 클러스터를 짓는 등 대규모 AI 인프라를 구축하는 ‘스타게이트’ 프로젝트 해외 확장 프로그램이다.\n",
      "이 프로그램을 통해 오픈AI는 각국 정부 및 산업 파트너와 함께 데이터 주권, 현지 인프라 구축, AI 모델 현지화, 스타트업 생태계 육성 등을 포괄적으로 지원한다..\n",
      "NSP 예측: 문장 연결 X\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs = \"\"\"\n",
    "오픈AI가 한국에 법인을 설립하고 국내 공식 진출한다. 인공지능(AI) 기술 수용성과 산업 잠재력이 높은 한국을 글로벌 협력의 핵심 거점으로 삼고, 정부 및 기업과의 인프라 구축·모델 개발 협력에 박차를 가하겠다는 방침이다.\n",
    "\n",
    "한국을 찾은 제이슨 권 오픈AI 최고전략책임자(CSO)는 26일 서울 종로구 포시즌스 호텔에서 기자들과 만나 향후 몇 달 내 서울에 한국 법인을 설립하고, 각국 현지에서 AI 생태계 협력을 강화하는 ‘오픈AI 포 컨트리즈’ 프로그램을 한국에 적용하겠다는 계획을 밝혔다.\n",
    "\n",
    "서울에 설립되는 오픈AI 한국 법인은 글로벌 11번째 지사로 아시아에서는 일본과 싱가포르에 이어 세 번째 사무소다. 오픈AI는 한국 고객 지원, 기업 협업, 공공 파트너십 등을 위한 현지 인력을 채용할 계획이다. 구체적인 인력 규모 등은 공개되지 않았으나 곧 구직 페이지에 공지될 예정이다.\n",
    "\n",
    "권 CSO는 “한국은 AI 기술 수용 속도가 세계 최고 수준으로 미국 다음으로 챗GPT 유료 구독자를 많이 보유한 국가이며, 특히 서울은 영상 생성 모델 ‘소라’ 사용률이 전 세계 1위인 도시”라며 “현지 법인을 중심으로 파트너십을 강화하고 한국 맞춤형 AI를 함께 만들고자 한다”고 말했다.\n",
    "\n",
    "오픈AI는 한국 법인 설립과 동시에 최근 전세계 각국에서 추진하고 있는 ‘오픈AI 포 컨트리즈’ 프로그램도 한국에 본격 적용할 방침이다. 오픈AI 포 컨트리즈는 아랍에미리트(UAE) 아부다비에 5기가와트(GW) 규모의 데이터 센터 클러스터를 짓는 등 대규모 AI 인프라를 구축하는 ‘스타게이트’ 프로젝트 해외 확장 프로그램이다. 이 프로그램을 통해 오픈AI는 각국 정부 및 산업 파트너와 함께 데이터 주권, 현지 인프라 구축, AI 모델 현지화, 스타트업 생태계 육성 등을 포괄적으로 지원한다.\n",
    "\"\"\"\n",
    "\n",
    "paragraph_list = split_sentences(paragraphs)\n",
    "for i in range(len(paragraph_list) -1):\n",
    "    print(paragraph_list[i])\n",
    "    print(paragraph_list[i+1])\n",
    "    inference_nsp(paragraph_list[i],paragraph_list[i+1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00692bc1",
   "metadata": {},
   "source": [
    "### mlm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d6ad6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def inference_mlm(masked_sentence):\n",
    "    # 토크나이징 (subword 단위)\n",
    "    token_ids = vocab.encode_as_ids(masked_sentence)\n",
    "\n",
    "    # [MASK] 토큰 위치 찾기\n",
    "    mask_index = token_ids.index(mask_id)\n",
    "\n",
    "    # segment는 모두 0 (문장 하나만 있으므로)\n",
    "    segments = [0] * len(token_ids)\n",
    "\n",
    "    # padding\n",
    "    input_ids = pad_sequences([token_ids], maxlen=config.n_seq, padding='post')\n",
    "    segment_ids = pad_sequences([segments], maxlen=config.n_seq, padding='post')\n",
    "\n",
    "    # 추론\n",
    "    _, outputs_mlm = pre_train_model.predict([input_ids, segment_ids], verbose=0)\n",
    "\n",
    "    # [MASK] 위치의 vocab 예측\n",
    "    predicted_id = np.argmax(outputs_mlm[0][mask_index])\n",
    "    predicted_token = vocab.id_to_piece(int(predicted_id))\n",
    "\n",
    "    return predicted_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acc6e4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁진출'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"오픈AI는 [MASK]에 진출했다.\"\n",
    "\n",
    "inference_mlm(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f74f1f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문: 오픈AI가 한국에 법인을 설립하고 국내 공식 진출한다.\n",
      "토큰화: ['▁오', '픈', 'A', 'I', '가', '▁한국', '에', '▁법', '인을', '▁설립', '하고', '▁국내', '▁공식', '▁진출', '한다', '.']\n",
      "마스킹 위치: 4 (마스킹된 토큰: 가)\n",
      "예측된 토큰: ▁등\n",
      "----------------------------------------\n",
      "원문: 인공지능(AI) 기술 수용성과 산업 잠재력이 높은 한국을 글로벌 협력의 핵심 거점으로 삼고, 정부 및 기업과의 인프라 구축·모델 개발 협력에 박차를 가하겠다는 방침이다.\n",
      "토큰화: ['▁인', '공', '지', '능', '(', 'A', 'I', ')', '▁기술', '▁수용', '성과', '▁산업', '▁잠', '재', '력이', '▁높은', '▁한국', '을', '▁글', '로', '벌', '▁협', '력의', '▁핵', '심', '▁거', '점으로', '▁삼', '고', ',', '▁정부', '▁및', '▁기업', '과의', '▁인', '프', '라', '▁구축', '·', '모', '델', '▁개발', '▁협', '력에', '▁박', '차를', '▁가', '하', '겠', '다는', '▁방', '침', '이다', '.']\n",
      "마스킹 위치: 28 (마스킹된 토큰: 고)\n",
      "예측된 토큰: ,\n",
      "----------------------------------------\n",
      "원문: 한국을 찾은 제이슨 권 오픈AI 최고전략책임자(CSO)는 26일 서울 종로구 포시즌스 호텔에서 기자들과 만나 향후 몇 달 내 서울에 한국 법인을 설립하고, 각국 현지에서 AI 생태계 협력을 강화하는 ‘오픈AI 포 컨트리즈’ 프로그램을 한국에 적용하겠다는 계획을 밝혔다.\n",
      "토큰화: ['▁한국', '을', '▁찾', '은', '▁제', '이', '슨', '▁권', '▁오', '픈', 'A', 'I', '▁최고', '전', '략', '책', '임', '자', '(', 'C', 'S', 'O', ')', '는', '▁26', '일', '▁서울', '▁종', '로', '구', '▁포', '시즌', '스', '▁호', '텔', '에서', '▁기', '자', '들과', '▁만나', '▁향', '후', '▁몇', '▁달', '▁내', '▁서울', '에', '▁한국', '▁법', '인을', '▁설립', '하고', ',', '▁각', '국', '▁현', '지에서', '▁A', 'I', '▁생', '태', '계', '▁협', '력을', '▁강화', '하는', '▁‘', '오', '픈', 'A', 'I', '▁포', '▁컨', '트', '리즈', '’', '▁프로그램', '을', '▁한국', '에', '▁적용', '하', '겠', '다는', '▁계획', '을', '▁밝혔다', '.']\n",
      "마스킹 위치: 70 (마스킹된 토큰: I)\n",
      "예측된 토큰: ▁및\n",
      "----------------------------------------\n",
      "원문: 서울에 설립되는 오픈AI 한국 법인은 글로벌 11번째 지사로 아시아에서는 일본과 싱가포르에 이어 세 번째 사무소다.\n",
      "토큰화: ['▁서울', '에', '▁설립', '되는', '▁오', '픈', 'A', 'I', '▁한국', '▁법', '인은', '▁글', '로', '벌', '▁11', '번째', '▁지', '사로', '▁아시아', '에서는', '▁일본', '과', '▁싱', '가', '포르', '에', '▁이어', '▁세', '▁번째', '▁사무', '소', '다', '.']\n",
      "마스킹 위치: 16 (마스킹된 토큰: ▁지)\n",
      "예측된 토큰: ▁번째\n",
      "----------------------------------------\n",
      "원문: 오픈AI는 한국 고객 지원, 기업 협업, 공공 파트너십 등을 위한 현지 인력을 채용할 계획이다.\n",
      "토큰화: ['▁오', '픈', 'A', 'I', '는', '▁한국', '▁고', '객', '▁지원', ',', '▁기업', '▁협', '업', ',', '▁공공', '▁파', '트', '너', '십', '▁등을', '▁위한', '▁현', '지', '▁인', '력을', '▁채', '용', '할', '▁계획', '이다', '.']\n",
      "마스킹 위치: 3 (마스킹된 토큰: I)\n",
      "예측된 토큰: ▁등을\n",
      "----------------------------------------\n",
      "원문: 구체적인 인력 규모 등은 공개되지 않았으나 곧 구직 페이지에 공지될 예정이다.\n",
      "토큰화: ['▁구체', '적인', '▁인', '력', '▁규모', '▁등은', '▁공개', '되지', '▁않았', '으나', '▁곧', '▁구', '직', '▁페', '이지', '에', '▁공', '지', '될', '▁예정', '이다', '.']\n",
      "마스킹 위치: 15 (마스킹된 토큰: 에)\n",
      "예측된 토큰: ▁않고\n",
      "----------------------------------------\n",
      "원문: 권 CSO는 “한국은 AI 기술 수용 속도가 세계 최고 수준으로 미국 다음으로 챗GPT 유료 구독자를 많이 보유한 국가이며, 특히 서울은 영상 생성 모델 ‘소라’ 사용률이 전 세계 1위인 도시”라며 “현지 법인을 중심으로 파트너십을 강화하고 한국 맞춤형 AI를 함께 만들고자 한다”고 말했다.\n",
      "토큰화: ['▁권', '▁C', 'S', 'O', '는', '▁“', '한', '국은', '▁A', 'I', '▁기술', '▁수용', '▁속', '도가', '▁세계', '▁최고', '▁수준', '으로', '▁미국', '▁다음', '으로', '▁', '챗', 'G', 'P', 'T', '▁유', '료', '▁구', '독', '자를', '▁많이', '▁보유', '한', '▁국가', '이며', ',', '▁특히', '▁서울', '은', '▁영', '상', '▁생성', '▁모델', '▁‘', '소', '라', '’', '▁사용', '률', '이', '▁전', '▁세계', '▁1', '위', '인', '▁도시', '”', '라', '며', '▁“', '현', '지', '▁법', '인을', '▁중심으로', '▁파', '트', '너', '십', '을', '▁강화', '하고', '▁한국', '▁맞', '춤', '형', '▁A', 'I', '를', '▁함께', '▁만들', '고', '자', '▁한다', '”', '고', '▁말했다', '.']\n",
      "마스킹 위치: 57 (마스킹된 토큰: ”)\n",
      "예측된 토큰: ▁등을\n",
      "----------------------------------------\n",
      "원문: 오픈AI는 한국 법인 설립과 동시에 최근 전세계 각국에서 추진하고 있는 ‘오픈AI 포 컨트리즈’ 프로그램도 한국에 본격 적용할 방침이다.\n",
      "토큰화: ['▁오', '픈', 'A', 'I', '는', '▁한국', '▁법', '인', '▁설립', '과', '▁동시에', '▁최근', '▁전', '세계', '▁각', '국', '에서', '▁추진', '하고', '▁있는', '▁‘', '오', '픈', 'A', 'I', '▁포', '▁컨', '트', '리즈', '’', '▁프로그램', '도', '▁한국', '에', '▁본격', '▁적용', '할', '▁방', '침', '이다', '.']\n",
      "마스킹 위치: 37 (마스킹된 토큰: ▁방)\n",
      "예측된 토큰: ▁수\n",
      "----------------------------------------\n",
      "원문: 오픈AI 포 컨트리즈는 아랍에미리트(UAE) 아부다비에 5기가와트(GW) 규모의 데이터 센터 클러스터를 짓는 등 대규모 AI 인프라를 구축하는 ‘스타게이트’ 프로젝트 해외 확장 프로그램이다.\n",
      "토큰화: ['▁오', '픈', 'A', 'I', '▁포', '▁컨', '트', '리즈', '는', '▁아랍', '에', '미', '리트', '(', 'U', 'A', 'E', ')', '▁아', '부', '다', '비', '에', '▁5', '기가', '와', '트', '(', 'G', 'W', ')', '▁규모', '의', '▁데이터', '▁센터', '▁클', '러', '스터', '를', '▁짓', '는', '▁등', '▁대규모', '▁A', 'I', '▁인', '프', '라', '를', '▁구축', '하는', '▁‘', '스타', '게', '이트', '’', '▁프로젝', '트', '▁해외', '▁확장', '▁프로그램', '이다', '.']\n",
      "마스킹 위치: 21 (마스킹된 토큰: 비)\n",
      "예측된 토큰: ▁게임\n",
      "----------------------------------------\n",
      "원문: 이 프로그램을 통해 오픈AI는 각국 정부 및 산업 파트너와 함께 데이터 주권, 현지 인프라 구축, AI 모델 현지화, 스타트업 생태계 육성 등을 포괄적으로 지원한다..\n",
      "토큰화: ['▁이', '▁프로그램', '을', '▁통해', '▁오', '픈', 'A', 'I', '는', '▁각', '국', '▁정부', '▁및', '▁산업', '▁파', '트', '너', '와', '▁함께', '▁데이터', '▁주', '권', ',', '▁현', '지', '▁인', '프', '라', '▁구축', ',', '▁A', 'I', '▁모델', '▁현', '지', '화', ',', '▁스타', '트', '업', '▁생', '태', '계', '▁육', '성', '▁등을', '▁포', '괄', '적으로', '▁지원', '한다', '..']\n",
      "마스킹 위치: 19 (마스킹된 토큰: ▁데이터)\n",
      "예측된 토큰: ▁산업\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "mask_token = \"[MASK]\"\n",
    "mask_id = vocab.piece_to_id(mask_token)\n",
    "\n",
    "def mask_random_token(sentence):\n",
    "    # 문장을 subword 단위로 토크나이징\n",
    "    tokens = vocab.encode_as_pieces(sentence)\n",
    "    token_ids = vocab.encode_as_ids(sentence)\n",
    "\n",
    "    # [MASK] 불가능한 경우 건너뛰기\n",
    "    if len(token_ids) < 3:\n",
    "        return None, None, None\n",
    "\n",
    "    # 랜덤 위치 선택 (단, [CLS]/[SEP] 같은 거 제외)\n",
    "    mask_idx = random.randint(0, len(token_ids) - 1)\n",
    "\n",
    "    # 마스킹된 input_ids 생성\n",
    "    masked_ids = token_ids.copy()\n",
    "    masked_ids[mask_idx] = mask_id\n",
    "\n",
    "    return tokens, masked_ids, mask_idx\n",
    "\n",
    "def inference_mlm_from_masked_ids(masked_ids, mask_idx):\n",
    "    # segment는 모두 0 (single sentence)\n",
    "    segments = [0] * len(masked_ids)\n",
    "\n",
    "    # padding\n",
    "    input_ids = pad_sequences([masked_ids], maxlen=config.n_seq, padding='post')\n",
    "    segment_ids = pad_sequences([segments], maxlen=config.n_seq, padding='post')\n",
    "\n",
    "    # 추론\n",
    "    _, mlm_logits = pre_train_model.predict([input_ids, segment_ids], verbose=0)\n",
    "\n",
    "    # 해당 위치의 예측 결과\n",
    "    predicted_id = int(np.argmax(mlm_logits[0][mask_idx]))\n",
    "    predicted_token = vocab.id_to_piece(predicted_id)\n",
    "\n",
    "    return predicted_token\n",
    "\n",
    "# 문장 리스트 순회\n",
    "for sent in paragraph_list:\n",
    "    tokens, masked_ids, mask_idx = mask_random_token(sent)\n",
    "    if tokens is None:\n",
    "        continue\n",
    "\n",
    "    predicted_token = inference_mlm_from_masked_ids(masked_ids, mask_idx)\n",
    "\n",
    "    print(\"원문:\", sent)\n",
    "    print(\"토큰화:\", tokens)\n",
    "    print(\"마스킹 위치:\", mask_idx, f\"(마스킹된 토큰: {tokens[mask_idx]})\")\n",
    "    print(\"예측된 토큰:\", predicted_token)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f4e9ac",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- mlm의 loss는 줄어들지 않는 것으로 보인다. \n",
    "- mlm 추론을 테스트 해 본 결과, 문장에 있는 다른 토큰이 반복되서 예측 된 것을 확인했다. \n",
    "    - `산업` 파트너와 함께 `데이터` 주권 -> 예측해야 하는 토큰는 `데이터` 예측한 토큰 `산업`\n",
    "    - 오픈A`I`는 한국 고객 지원, 기업 협업, 공공 파트너십 `등을` -> `I`, 예측한 토큰 `등을`  \n",
    "    \n",
    "=> mlm 예측 성능은 좋지 않은 것 같다. 충분한 학습이 이루어지지 않은 것 같은데, 학습에 사용된 데이터 수와 vocab 수가 적어서 그런 것 같다. \n",
    "    \n",
    "\n",
    "- nsp의 loss는 줄어들다가 수렴하는 것으로 보인다.  \n",
    "- 하지만 실제 nsp 추론 테스트 해 본 결과, 다음 문장의 예측정도가 반반으로 보인다.   \n",
    "\n",
    "=> 더 충분한 테스트가 필요하지만, 지금 데이터로는 예측을 잘 못한다고 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce3284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
