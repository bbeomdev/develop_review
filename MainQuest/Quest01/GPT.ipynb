{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa76425",
   "metadata": {},
   "source": [
    "# GPT 모델을 구성해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbadd24f",
   "metadata": {},
   "source": [
    "## Transformer와 비교해 변경이 필요한 부분 서술\n",
    "\n",
    "- GPT 모델에서는 Decoder만 사용하므로, Encoder의 모든 부분은 삭제한다.\n",
    "- Decoder의 input에서 positional encoding을 positional embedding으로 변경한다.(positional encoding은 sin, cos으로 고정, positiona embedding은 학습 가능하다)\n",
    "- Encoder가 없으므로 encoder-decoder attension layer를 제거한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c9c712",
   "metadata": {},
   "source": [
    "model summary. \n",
    "![summary](./model_summary.png)\n",
    "model fit. \n",
    "![fit](./model_fit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077bb23c",
   "metadata": {},
   "source": [
    "## 모델의 입력 형태에 맞게 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d3edb",
   "metadata": {},
   "source": [
    "### import labrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950f5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f46f3",
   "metadata": {},
   "source": [
    "### 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8830f547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>요즘 드라마가 땡겨</td>\n",
       "      <td>저도 드라마 좋아해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9962</th>\n",
       "      <td>사랑하는 사람이랑 결혼하고싶어</td>\n",
       "      <td>언젠가 그런 사람이 당신 옆에 있을거예요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7443</th>\n",
       "      <td>이별 중독</td>\n",
       "      <td>슬픈 단어네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7944</th>\n",
       "      <td>저는 개인적으로 따로 연락하자는분들.</td>\n",
       "      <td>부담스러운가봅니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6270</th>\n",
       "      <td>매번 왜 이렇게 남는게 후회인지 모르겠네 ㅎㅎ</td>\n",
       "      <td>모든 일에는 후회가 남기 마련인가봐요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Q                        A\n",
       "3476                 요즘 드라마가 땡겨             저도 드라마 좋아해요.\n",
       "9962           사랑하는 사람이랑 결혼하고싶어  언젠가 그런 사람이 당신 옆에 있을거예요.\n",
       "7443                      이별 중독                 슬픈 단어네요.\n",
       "7944       저는 개인적으로 따로 연락하자는분들.               부담스러운가봅니다.\n",
       "6270  매번 왜 이렇게 남는게 후회인지 모르겠네 ㅎㅎ    모든 일에는 후회가 남기 마련인가봐요."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = os.environ.get(\"HOME\") + \"/aiffel/transformer_chatbot/data/ChatbotData.csv\"\n",
    "df_data = pd.read_csv(data_path)\n",
    "df_data = df_data[['Q','A']]\n",
    "df_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0702be4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q    0\n",
       "A    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afb8ca44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>결혼이나 하지 왜 자꾸 나한테 화 내냐구!</td>\n",
       "      <td>힘들겠네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>결혼이나 하지 왜 자꾸 나한테 화 내냐구!</td>\n",
       "      <td>힘들겠네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>고백하고 후회하면 어떡하지</td>\n",
       "      <td>후회는 후회를 낳을뿐이에요. 용기 내세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5537</th>\n",
       "      <td>고백하고 후회하면 어떡하지</td>\n",
       "      <td>후회는 후회를 낳을뿐이에요. 용기 내세요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>공부는 내 체질이 아닌 것 같아</td>\n",
       "      <td>확신이 없나봐요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8780</th>\n",
       "      <td>회사 사람들이 아직도 불편해</td>\n",
       "      <td>회사에는 동료가 있을 뿐이에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5232</th>\n",
       "      <td>회사에는 왜 친구 같은 사람이 없을까</td>\n",
       "      <td>회사는 친구 사귀는 곳이 아니에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>회사에는 왜 친구 같은 사람이 없을까</td>\n",
       "      <td>회사는 친구 사귀는 곳이 아니에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>후련하달까</td>\n",
       "      <td>후련하니 다행이에요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8789</th>\n",
       "      <td>후련하달까</td>\n",
       "      <td>후련하니 다행이에요.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Q                        A\n",
       "152   결혼이나 하지 왜 자꾸 나한테 화 내냐구!                   힘들겠네요.\n",
       "5527  결혼이나 하지 왜 자꾸 나한테 화 내냐구!                   힘들겠네요.\n",
       "189            고백하고 후회하면 어떡하지  후회는 후회를 낳을뿐이에요. 용기 내세요.\n",
       "5537           고백하고 후회하면 어떡하지  후회는 후회를 낳을뿐이에요. 용기 내세요.\n",
       "226         공부는 내 체질이 아닌 것 같아                확신이 없나봐요.\n",
       "...                       ...                      ...\n",
       "8780          회사 사람들이 아직도 불편해        회사에는 동료가 있을 뿐이에요.\n",
       "5232     회사에는 왜 친구 같은 사람이 없을까      회사는 친구 사귀는 곳이 아니에요.\n",
       "8782     회사에는 왜 친구 같은 사람이 없을까      회사는 친구 사귀는 곳이 아니에요.\n",
       "5246                    후련하달까              후련하니 다행이에요.\n",
       "8789                    후련하달까              후련하니 다행이에요.\n",
       "\n",
       "[146 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = df_data[df_data.duplicated( keep=False)]\n",
    "duplicates.sort_values(by=['Q','A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b37f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q와 A 모두 중복인 경우 마지막 행을 drop\n",
    "df_unique = df_data.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3083df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Q, A]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인\n",
    "df_unique[df_unique.duplicated( keep=False)].sort_values(by=['Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65119e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 제거 전:  (11823, 2)\n",
      "중복 제거 후:  (11750, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"중복 제거 전: \",df_data.shape)\n",
    "print(\"중복 제거 후: \",df_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "623d459d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # 입력받은 sentence를 소문자로 변경하고 양쪽 공백을 제거\n",
    "    sentence = sentence.lower().strip()    \n",
    "\n",
    "    # 단어와 구두점 사이 거리 두기\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "\n",
    "    # 연속된 공백 -> 하나의 공백\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    # (a-z, A-Z, 0-9, 가-힣, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "    sentence = re.sub(r'[^a-zA-Z0-9가-힣\\.\\?\\!\\,]', ' ', sentence)\n",
    "    \n",
    "    # 양쪽 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aee36903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2132b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed['Q'] = df_unique['Q'].apply(preprocess_sentence)\n",
    "df_preprocessed['A'] = df_unique['A'].apply(preprocess_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e042eec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡 !</td>\n",
       "      <td>하루가 또 가네요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ppl 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q             A\n",
       "0          12시 땡 !   하루가 또 가네요 .\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다 .\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠 .\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠 .\n",
       "4          ppl 심하네   눈살이 찌푸려지죠 ."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80eef04",
   "metadata": {},
   "source": [
    "numpy 배열로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b93f077",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df_preprocessed['Q'].values\n",
    "answers = df_preprocessed['A'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e7b6fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11750,)\n",
      "<class 'numpy.ndarray'>\n",
      "12시 땡 !\n",
      "(11750,)\n",
      "<class 'numpy.ndarray'>\n",
      "하루가 또 가네요 .\n"
     ]
    }
   ],
   "source": [
    "print(questions.shape)\n",
    "print(type(questions))\n",
    "print(questions[0])\n",
    "print(answers.shape)\n",
    "print(type(answers))\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05078768",
   "metadata": {},
   "source": [
    "### tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ca4cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# SubwordTextEncoderfh 토크나이징, questions와 answers를 더한 말뭉치를 기반으로 학습하여 토크나이징\n",
    "# 토큰의 갯수는 2**13 = 8192개 를 만들도록 시도\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions+answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd1ea6d",
   "metadata": {},
   "source": [
    "입력을 시퀀스 형태로 만들기 위해 DELIMETER 토큰을 추가한다   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a092f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_token, end_token 추가; 위에서 8192[0~8191]개의 토큰이 생성을 시도했고, \n",
    "# 그 다음 인텍스로 start_token, end_token을 지정\n",
    "# []를 추가하는 이유? 토큰 시퀀스와 +연산으로 붙이기 편하게 하기 위해(리스트 합)\n",
    "\n",
    "START_TOKEN, DELIMETER, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1], [tokenizer.vocab_size+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94e5fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN:  [8318]\n",
      "DELIMETER:  [8319]\n",
      "END_TOKEN:  [8320]\n"
     ]
    }
   ],
   "source": [
    "# 2**13개 토큰 시도 결과로 8318개 만들어짐\n",
    "print(\"START_TOKEN: \",START_TOKEN)\n",
    "print(\"DELIMETER: \", DELIMETER)\n",
    "print(\"END_TOKEN: \", END_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8636c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8321\n"
     ]
    }
   ],
   "source": [
    "# 단어장 크기 설정 -> start_token, delimeter, end_token 추가\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 3\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89e555a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5821, 605, 2491, 4166]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2682, 7632, 9, 6351, 94, 1]\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징 확인, 인코딩 확인\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46138f46",
   "metadata": {},
   "source": [
    "### 문장 길이 분포 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0122592",
   "metadata": {},
   "source": [
    "### decoder input\n",
    "\n",
    "sos + question + delimeter + answer + eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8b16bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for question, answer in zip(questions, answers):\n",
    "    sequence = START_TOKEN + tokenizer.encode(question) \\\n",
    "                    + DELIMETER + tokenizer.encode(answer) \\\n",
    "                    + END_TOKEN\n",
    "    input_sequences.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30611704",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(seq) for seq in input_sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69d5630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 길이: 5\n",
      "최대 길이: 41\n",
      "평균 길이: 14.34136170212766\n",
      "중앙값 길이: 14.0\n",
      "95퍼센타일 길이: 22.0\n"
     ]
    }
   ],
   "source": [
    "print(\"최소 길이:\", np.min(sequence_lengths))\n",
    "print(\"최대 길이:\", np.max(sequence_lengths))\n",
    "print(\"평균 길이:\", np.mean(sequence_lengths))\n",
    "print(\"중앙값 길이:\", np.median(sequence_lengths))\n",
    "print(\"95퍼센타일 길이:\", np.percentile(sequence_lengths, 95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34013705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95퍼센타일 길이: 26.0\n"
     ]
    }
   ],
   "source": [
    "print(\"95퍼센타일 길이:\", np.percentile(sequence_lengths, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75693bf0",
   "metadata": {},
   "source": [
    "26으로 MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b950294",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12e51f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filtered = []\n",
    "for sequence in input_sequences:\n",
    "    if len(sequence) <= 26:\n",
    "        input_filtered.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da1b23f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 전:  11750\n",
      "필터링 후:  11666\n"
     ]
    }
   ],
   "source": [
    "print(\"필터링 전: \",len(input_sequences))\n",
    "print(\"필터링 후: \", len(input_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d3b53",
   "metadata": {},
   "source": [
    "### 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf4f5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      input_sequences, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933859bf",
   "metadata": {},
   "source": [
    "### 데이터 셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ce50b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 번에 모델에 입력될 데이터 샘플 수: 64개를 한 묶음으로 보고 병렬처리\n",
    "BATCH_SIZE = 64\n",
    "# 셔플을 위한 버퍼 크기 전체 데이터 중 BUFFER_SIZE만큼 메모리에 올려서 셔플하겠다는 것; 메모리 사용량 관리 필요\n",
    "# 총 데이터가 11750개 정도라...그냥 섞는 메모리 공간을 좀 크게 함.\n",
    "BUFFER_SIZE = 15000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'dec_inputs': input_padded[:, :-1],\n",
    "    },\n",
    "    {\n",
    "        'outputs': input_padded[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()  # 처음에 메모리에 로딩하고, 이후부터 디스크 I/O 없이 하겠다.\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)   # 섞어\n",
    "dataset = dataset.batch(BATCH_SIZE)      # 배치사이즈로 배치 묶음 만들어\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)    # 다음 배치를 백그라운드에서 미리 준비해서 훈련이 끊기지 않도록 함. AUTOTUNE은 TensorFlow가 최적의 prefetch 양을 자동으로 조절해줌 → 성능 최적화\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa033f27",
   "metadata": {},
   "source": [
    "## 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4adf91d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, max_length, embedding_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.pos_embedding = layers.Embedding(\n",
    "            input_dim=max_length,\n",
    "            output_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch_size, seq_len, embedding_dim)\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        positions = tf.range(start=0, limit=seq_len, delta=1)  # (seq_len,)\n",
    "        positions = tf.expand_dims(positions, 0)               # (1, seq_len)\n",
    "        positions = tf.tile(positions, [batch_size, 1])        # (batch_size, seq_len)\n",
    "\n",
    "        pos_embeddings = self.pos_embedding(positions)         # (batch_size, seq_len, embedding_dim)\n",
    "        return inputs + pos_embeddings                         # broadcasting-safe 덧셈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1922597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    \"\"\"\n",
    "    query와 key의 내적으로 유사도를 구해 -> 행렬로 나옴(셀프어텐션에서만 정사각행렬임)\n",
    "    코사인 유사도랑 비슷함\n",
    "    소프트맥스가 너무 커지지 않기 위해 루트(차원)으로 나누어 줌\n",
    "    소프트맥스를 통과하고 V를 행렬곱\n",
    "    \"\"\"\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += mask * -1e9\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a03be21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    어텐션을 차원 그룹으로 나누어서 병렬로 수행\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # 하나의 입력으로 부터 3개의 표현을 뽑아내기 위해 각각 다른 dense를 사용\n",
    "        # 입력에 대해서 각각 다른 Dense layer를 통과시켜 서로 다른 관점의 벡터로 선형변환 시킴\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = (\n",
    "            inputs[\"query\"],\n",
    "            inputs[\"key\"],\n",
    "            inputs[\"value\"],\n",
    "            inputs[\"mask\"],\n",
    "        )\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        \n",
    "        # (batch_size, num_heads, seq_len_q, depth) -> (batch_size, seq_len_q, num_heads, depth)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36029fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩이 어텐션 계산이나 손실 계산에 영향을 주지 않도록 마스킹\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adec34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더에서 사용. Q, K 유사도 계산에서, 현재보다 미래 위치 토큰을 가리지 위해 사용.(예측해야 되니까)\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cb35472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    \n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "#     padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(d_model, num_heads, name=\"attention_1\")(\n",
    "        inputs={\n",
    "            \"query\": inputs,\n",
    "            \"key\": inputs,\n",
    "            \"value\": inputs,\n",
    "            \"mask\": look_ahead_mask,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    \n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation=\"relu\")(attention1)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(outputs + attention1)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs,  look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5870e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    \n",
    "    look_ahead_mask = tf.keras.Input(shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "\n",
    "    # 패딩 마스크\n",
    "#     padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEmbedding(MAX_LENGTH, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"decoder_layer_{}\".format(i),\n",
    "        )([outputs,  look_ahead_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs,  look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5135fbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT(\n",
    "    vocab_size, num_layers, units, d_model, num_heads, dropout, name=\"gpt\"\n",
    "):\n",
    "\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask, output_shape=(1, None, None), name=\"look_ahead_mask\"\n",
    "    )(dec_inputs)\n",
    "\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs,  look_ahead_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ca05365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gpt\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    23209472    dec_inputs[0][0]                 \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8321)   4268673     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 27,478,145\n",
      "Trainable params: 27,478,145\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 12 # 디코더의 층의 개수\n",
    "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "929bb560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블인 시퀀스에 패딩이 되어 있으므로, loss를 계산할 때 패딩 마스크를 적용해야 합니다.\n",
    "# def loss_function(y_true, y_pred):\n",
    "#     y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "#     loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "#         from_logits=True, reduction=\"none\"\n",
    "#     )(y_true, y_pred)\n",
    "\n",
    "#     mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "#     loss = tf.multiply(loss, mask)\n",
    "\n",
    "#     return tf.reduce_mean(loss)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    # y_true: (batch, seq_len)\n",
    "    # y_pred: (batch, seq_len, vocab_size)\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )(y_true, y_pred)\n",
    "\n",
    "    # 패딩 마스킹: 0인 부분 제외\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "730fa8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률을 train step에 따라 변화를 줌\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c29f2502",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# def accuracy(y_true, y_pred):\n",
    "#   y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "#   return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c412170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "184/184 [==============================] - 54s 130ms/step - loss: 8.0808 - accuracy: 0.0413\n",
      "Epoch 2/50\n",
      "184/184 [==============================] - 25s 136ms/step - loss: 6.7889 - accuracy: 0.1079\n",
      "Epoch 3/50\n",
      "184/184 [==============================] - 25s 135ms/step - loss: 6.2801 - accuracy: 0.1194\n",
      "Epoch 4/50\n",
      "184/184 [==============================] - 24s 132ms/step - loss: 5.9622 - accuracy: 0.1245\n",
      "Epoch 5/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 5.7566 - accuracy: 0.1281\n",
      "Epoch 6/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 5.5958 - accuracy: 0.1322\n",
      "Epoch 7/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 5.4430 - accuracy: 0.1369\n",
      "Epoch 8/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 5.3017 - accuracy: 0.1407\n",
      "Epoch 9/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 5.1478 - accuracy: 0.1448\n",
      "Epoch 10/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 4.9907 - accuracy: 0.1492\n",
      "Epoch 11/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 4.8305 - accuracy: 0.1535\n",
      "Epoch 12/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 4.6788 - accuracy: 0.1576\n",
      "Epoch 13/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 4.5370 - accuracy: 0.1613\n",
      "Epoch 14/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 4.3888 - accuracy: 0.1653\n",
      "Epoch 15/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 4.2438 - accuracy: 0.1702\n",
      "Epoch 16/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 4.1091 - accuracy: 0.1742\n",
      "Epoch 17/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 3.9732 - accuracy: 0.1801\n",
      "Epoch 18/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 3.8747 - accuracy: 0.1830\n",
      "Epoch 19/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 3.7555 - accuracy: 0.1882\n",
      "Epoch 20/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 3.6654 - accuracy: 0.1921\n",
      "Epoch 21/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 3.5927 - accuracy: 0.1953\n",
      "Epoch 22/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 3.5742 - accuracy: 0.1947\n",
      "Epoch 23/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 3.4058 - accuracy: 0.2048\n",
      "Epoch 24/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 3.4563 - accuracy: 0.2065\n",
      "Epoch 25/50\n",
      "184/184 [==============================] - 25s 134ms/step - loss: 3.8861 - accuracy: 0.1780\n",
      "Epoch 26/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 3.1701 - accuracy: 0.2190\n",
      "Epoch 27/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 2.9325 - accuracy: 0.2367\n",
      "Epoch 28/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 2.8259 - accuracy: 0.2444\n",
      "Epoch 29/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 2.6792 - accuracy: 0.2576\n",
      "Epoch 30/50\n",
      "184/184 [==============================] - 25s 133ms/step - loss: 2.5441 - accuracy: 0.2698\n",
      "Epoch 31/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 2.3881 - accuracy: 0.2841\n",
      "Epoch 32/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 2.2802 - accuracy: 0.2928\n",
      "Epoch 33/50\n",
      "184/184 [==============================] - 24s 132ms/step - loss: 2.1718 - accuracy: 0.3043\n",
      "Epoch 34/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 2.0733 - accuracy: 0.3143\n",
      "Epoch 35/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 2.0240 - accuracy: 0.3191\n",
      "Epoch 36/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.9951 - accuracy: 0.3218\n",
      "Epoch 37/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.8327 - accuracy: 0.3397\n",
      "Epoch 38/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.8120 - accuracy: 0.3416\n",
      "Epoch 39/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.8677 - accuracy: 0.3356\n",
      "Epoch 40/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.8101 - accuracy: 0.3433\n",
      "Epoch 41/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.8246 - accuracy: 0.3417\n",
      "Epoch 42/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.6862 - accuracy: 0.3575\n",
      "Epoch 43/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.6967 - accuracy: 0.3560\n",
      "Epoch 44/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.5962 - accuracy: 0.3673\n",
      "Epoch 45/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.6101 - accuracy: 0.3659\n",
      "Epoch 46/50\n",
      "184/184 [==============================] - 24s 132ms/step - loss: 1.5795 - accuracy: 0.3690\n",
      "Epoch 47/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.5252 - accuracy: 0.3750\n",
      "Epoch 48/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.5109 - accuracy: 0.3778\n",
      "Epoch 49/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.4475 - accuracy: 0.3860\n",
      "Epoch 50/50\n",
      "184/184 [==============================] - 24s 133ms/step - loss: 1.4335 - accuracy: 0.3873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x6ffd3c076070>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e2ec2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(input_text):\n",
    "    input_ids = START_TOKEN + tokenizer.encode(preprocess_sentence(input_text)) + DELIMETER\n",
    "    output_sequence = tf.expand_dims(input_ids, axis=0)  # shape: (1, seq_len)\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = model(output_sequence, training=False)\n",
    "        next_token_logits = predictions[:, -1, :]\n",
    "        predicted_id = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "        if predicted_id[0] == END_TOKEN[0]:\n",
    "            break\n",
    "\n",
    "        predicted_id = tf.expand_dims(predicted_id, axis=-1)  # shape: (1, 1)\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a8574187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size]\n",
    "    )\n",
    "\n",
    "    print(\"입력 : {}\".format(sentence))\n",
    "    print(\"출력 : {}\".format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4915aead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 퇴사하고 싶어\n",
      "출력 : 퇴사하고 싶어제가 드리고 싶네요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'퇴사하고 싶어제가 드리고 싶네요 .'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"퇴사하고 싶어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c7ae8638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 퇴사만세\n",
      "출력 : 퇴사만세더 열심히 하면 좋겠어요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'퇴사만세더 열심히 하면 좋겠어요 .'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"퇴사만세\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdd24f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 퇴사는게 좋을까?\n",
      "출력 : 퇴사는게 좋을까 ?좀 더 많이 다질거 같아요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'퇴사는게 좋을까 ?좀 더 많이 다질거 같아요 .'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"퇴사는게 좋을까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9f9a8d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 퇴사해도 될까?\n",
      "출력 : 퇴사해도 될까 ?전혀 갈 수 있을 거예요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'퇴사해도 될까 ?전혀 갈 수 있을 거예요 .'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"퇴사해도 될까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fa3d2d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 쉬고 싶어\n",
      "출력 : 쉬고 싶어내려 놓으세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'쉬고 싶어내려 놓으세요 .'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation(\"쉬고 싶어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c704813b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dd50b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007dbc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051e66c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
